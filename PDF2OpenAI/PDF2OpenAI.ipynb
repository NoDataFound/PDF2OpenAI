{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv, set_key\n",
    "import openai\n",
    "import ipywidgets as widgets\n",
    "from bs4 import BeautifulSoup\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai.api_key:\n",
    "    openai.api_key = input(\"Enter OPENAI_API_KEY API key\")\n",
    "    set_key(\".env\", \"OPENAI_API_KEY\", openai.api_key)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "input_dir = \"input/\"\n",
    "output_dir = \"output/\"\n",
    "text_dir = os.path.join(output_dir, \"text\")\n",
    "done_dir = os.path.join(output_dir, \"done\")\n",
    "for directory in [input_dir, output_dir, text_dir, done_dir]:\n",
    "  os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.mass.gov/lists/data-breach-reports\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "pdf_links = []\n",
    "for a_tag in soup.find_all(\"a\", href=True):\n",
    "    href = a_tag[\"href\"]\n",
    "    if href.startswith(\"https://www.mass.gov/doc/data-breach-report-20\"):\n",
    "        pdf_links.append(href)\n",
    "\n",
    "for url in pdf_links:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    filename = os.path.basename(url)\n",
    "    with open(os.path.join(input_dir, filename), \"wb\") as pdf_file:\n",
    "        pdf_file.write(response.content)\n",
    "    with open(os.path.join(input_dir, filename), \"rb\") as pdf_file:\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text = page.extract_text()\n",
    "\n",
    "            text_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            text_path = os.path.join(text_dir, text_filename)\n",
    "            with open(text_path, \"a\") as text_file:\n",
    "                text_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_options = {\n",
    "    'gpt-3.5-turbo': 'gpt-3.5-turbo',\n",
    "    'gpt-3.5-turbo-0301': 'gpt-3.5-turbo-0301',\n",
    "    'gpt-4': 'gpt-4',\n",
    "    'gpt-4-0314': 'gpt-4-0314',\n",
    "    'text-davinci-003': 'text-davinci-003',\n",
    "    'text-davinci-002': 'text-davinci-002',\n",
    "    'text-davinci-edit-001': 'text-davinci-edit-001',\n",
    "    'code-davinci-edit-001': 'code-davinci-edit-001'\n",
    "}\n",
    "\n",
    "dropdown = widgets.Dropdown(options=model_options)\n",
    "display(dropdown)\n",
    "\n",
    "selected_model = dropdown.value\n",
    "print(f'Selected model: {selected_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Authorization': f'Bearer {openai.api_key}'}\n",
    "openai_endpoint = f'https://api.openai.com/v1/engines/{dropdown.value}/completions'\n",
    "responses = []\n",
    "chunk_size = 2500\n",
    "for filename in os.listdir(text_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        text_path = os.path.join(text_dir, filename)\n",
    "        with open(text_path, \"r\") as text_file:\n",
    "            text = text_file.read()\n",
    "            for i in range(0, len(text), chunk_size):\n",
    "                chunk = text[i:i+chunk_size]\n",
    "                payload = {\n",
    "                    \"prompt\": chunk,\n",
    "                    \"max_tokens\": 2048,\n",
    "                    \"temperature\": 0.5,\n",
    "                    \"n\": 1,\n",
    "                    \"stop\": \"\\n\"\n",
    "                }\n",
    "                response = requests.post(openai_endpoint, headers=headers, json=payload)\n",
    "                response.raise_for_status()\n",
    "                responses.append(response.json())\n",
    "                print(response['choices'][0]['text'])\n",
    "\n",
    "# Combine responses and save to \"done\" directory\n",
    "output_text = \"\"\n",
    "for response in responses:\n",
    "    output_text += response['choices'][0]['text']\n",
    "\n",
    "done_path = os.path.join(done_dir, \"output.txt\")\n",
    "with open(done_path, \"w\") as done_file:\n",
    "    done_file.write(output_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
